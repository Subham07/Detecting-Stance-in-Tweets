{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('dataset_train.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>dear lord thank u for all of ur blessings forg...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>Blessed are the peacemakers, for they shall be...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>I am not conformed to this world. I am transfo...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>Salah should be prayed with #focus and #unders...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>And stay in your houses and do not display you...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   Target                                              Tweet   Stance\n",
       "0  101  Atheism  dear lord thank u for all of ur blessings forg...  AGAINST\n",
       "1  102  Atheism  Blessed are the peacemakers, for they shall be...  AGAINST\n",
       "2  103  Atheism  I am not conformed to this world. I am transfo...  AGAINST\n",
       "3  104  Atheism  Salah should be prayed with #focus and #unders...  AGAINST\n",
       "4  105  Atheism  And stay in your houses and do not display you...  AGAINST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_target1=data_train[data_train[\"Target\"]==\"Atheism\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_target2=data_train[data_train[\"Target\"]==\"Climate Change is a Real Concern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_target3=data_train[data_train[\"Target\"]==\"Feminist Movement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_target4=data_train[data_train[\"Target\"]==\"Hillary Clinton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_target5=data_train[data_train[\"Target\"]==\"Legalization of Abortion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for Pre-processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(docs): #list of strings\n",
    "    punctuations = '''!()[]{};:\"\\,<>./?@#$%^&*_~-'''\n",
    "    docs2=[];\n",
    "    i=0;\n",
    "    for sent in docs:\n",
    "        s=\"\"\n",
    "        for x in sent:\n",
    "            if(x not in punctuations):\n",
    "                s=s+x;\n",
    "        docs2.append(s);\n",
    "\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lowercase(docs): #list of strings\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        sent=sent.lower()\n",
    "        docs2.append(sent);\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(docs): #list of strings\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        sent=word_tokenize(sent)\n",
    "        docs2.append(sent);\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(docs): #list of list of words as param\n",
    "    stop_words=set(stopwords.words(\"english\"));\n",
    "\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        s=[];\n",
    "        for word in sent:\n",
    "            if(word not in stop_words):\n",
    "                s.append(word);\n",
    "        docs2.append(s);\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string(docs): #list of list of words as param\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        s=\"\"\n",
    "        s=' '.join(sent)\n",
    "        docs2.append(s);\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(docs): #list of list of words as param\n",
    "    lemmatizer=WordNetLemmatizer();\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        s=[];\n",
    "        for word in sent:\n",
    "            s.append(lemmatizer.lemmatize(word));\n",
    "        docs2.append(s);\n",
    "\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(docs): #list of strings\n",
    "    docs2=[]\n",
    "    for sent in docs:\n",
    "        sent2=sent.split(' ');\n",
    "        words=[]\n",
    "        for x in sent2:\n",
    "            flag=0;\n",
    "            for c in x:\n",
    "                if(c>='0' and c<='9'):\n",
    "                    flag=1;\n",
    "                    break;\n",
    "                else:\n",
    "                    pass;\n",
    "            if(flag==0):\n",
    "                words.append(x);\n",
    "        words=' '.join(words);\n",
    "        docs2.append(words)\n",
    "    docs=docs2;\n",
    "    docs2=[]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding length of training data for each Targets, Storing Training data in Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target1_train_len=len(data_train_target1)\n",
    "target2_train_len=len(data_train_target2)\n",
    "target3_train_len=len(data_train_target3)\n",
    "target4_train_len=len(data_train_target4)\n",
    "target5_train_len=len(data_train_target5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_train_data=[]\n",
    "for i in range(0,target1_train_len):\n",
    "    target1_train_data.append(data_train_target1[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2_train_data=[]\n",
    "for i in range(target1_train_len,target1_train_len+target2_train_len):\n",
    "    target2_train_data.append(data_train_target2[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target3_train_data=[]\n",
    "for i in range(target1_train_len+target2_train_len,target1_train_len+target2_train_len+target3_train_len):\n",
    "    target3_train_data.append(data_train_target3[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target4_train_data=[]\n",
    "for i in range(target1_train_len+target2_train_len+target3_train_len,target1_train_len+target2_train_len+target3_train_len+target4_train_len):\n",
    "    target4_train_data.append(data_train_target4[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target5_train_data=[]\n",
    "for i in range(target1_train_len+target2_train_len+target3_train_len+target4_train_len,target1_train_len+target2_train_len+target3_train_len+target4_train_len+target5_train_len):\n",
    "    target5_train_data.append(data_train_target5[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "print(len(target1_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of data for Target 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_train_data=convert_lowercase(target1_train_data)\n",
    "target1_train_data=remove_digits(target1_train_data)\n",
    "target1_train_data=remove_punctuations(target1_train_data)\n",
    "target1_train_data=tokenize_data(target1_train_data)\n",
    "target1_train_data=remove_stopwords(target1_train_data)\n",
    "target1_train_data=lemmatize(target1_train_data)\n",
    "target1_train_data=make_string(target1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear lord thank u ur blessing forgive sin lord give strength energy busy day ahead blessed hope semst',\n",
       " 'blessed peacemaker shall called child god matthew scripture peace semst',\n",
       " 'conformed world transformed renewing mind ispeaklife god semst',\n",
       " 'salah prayed focus understanding allah warns lazy prayer done show surah almaoon semst',\n",
       " 'stay house display like time ignorance quran semst']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1_train_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of data for Target 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2_train_data=convert_lowercase(target2_train_data)\n",
    "target2_train_data=remove_digits(target2_train_data)\n",
    "target2_train_data=remove_punctuations(target2_train_data)\n",
    "target2_train_data=tokenize_data(target2_train_data)\n",
    "target2_train_data=remove_stopwords(target2_train_data)\n",
    "target2_train_data=lemmatize(target2_train_data)\n",
    "target2_train_data=make_string(target2_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of data for Target 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target3_train_data=convert_lowercase(target3_train_data)\n",
    "target3_train_data=remove_digits(target3_train_data)\n",
    "target3_train_data=remove_punctuations(target3_train_data)\n",
    "target3_train_data=tokenize_data(target3_train_data)\n",
    "target3_train_data=remove_stopwords(target3_train_data)\n",
    "target3_train_data=lemmatize(target3_train_data)\n",
    "target3_train_data=make_string(target3_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of data for Target 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target4_train_data=convert_lowercase(target4_train_data)\n",
    "target4_train_data=remove_digits(target4_train_data)\n",
    "target4_train_data=remove_punctuations(target4_train_data)\n",
    "target4_train_data=tokenize_data(target4_train_data)\n",
    "target4_train_data=remove_stopwords(target4_train_data)\n",
    "target4_train_data=lemmatize(target4_train_data)\n",
    "target4_train_data=make_string(target4_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing of data for Target 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target5_train_data=convert_lowercase(target5_train_data)\n",
    "target5_train_data=remove_digits(target5_train_data)\n",
    "target5_train_data=remove_punctuations(target5_train_data)\n",
    "target5_train_data=tokenize_data(target5_train_data)\n",
    "target5_train_data=remove_stopwords(target5_train_data)\n",
    "target5_train_data=lemmatize(target5_train_data)\n",
    "target5_train_data=make_string(target5_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>He who exalts himself shall      be humbled; a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>#God is utterly powerless without Human interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID   Target                                              Tweet\n",
       "0  10001  Atheism  He who exalts himself shall      be humbled; a...\n",
       "1  10002  Atheism  RT @prayerbullets: I remove Nehushtan -previou...\n",
       "2  10003  Atheism  @Brainman365 @heidtjj @BenjaminLives I have so...\n",
       "3  10004  Atheism  #God is utterly powerless without Human interv...\n",
       "4  10005  Atheism  @David_Cameron   Miracles of #Multiculturalism..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test=pd.read_csv('dataset_test.csv',encoding = \"ISO-8859-1\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing test data for each Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_target1=data_test[data_test[\"Target\"]==\"Atheism\"]\n",
    "data_test_target2=data_test[data_test[\"Target\"]==\"Climate Change is a Real Concern\"]\n",
    "data_test_target3=data_test[data_test[\"Target\"]==\"Feminist Movement\"]\n",
    "data_test_target4=data_test[data_test[\"Target\"]==\"Hillary Clinton\"]\n",
    "data_test_target5=data_test[data_test[\"Target\"]==\"Legalization of Abortion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_test_len=len(data_test_target1)\n",
    "target2_test_len=len(data_test_target2)\n",
    "target3_test_len=len(data_test_target3)\n",
    "target4_test_len=len(data_test_target4)\n",
    "target5_test_len=len(data_test_target5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_test_data=[]\n",
    "for i in range(0,target1_test_len):\n",
    "    target1_test_data.append(data_test_target1[\"Tweet\"][i])\n",
    "    \n",
    "target2_test_data=[]\n",
    "for i in range(target1_test_len,target1_test_len+target2_test_len):\n",
    "    target2_test_data.append(data_test_target2[\"Tweet\"][i])\n",
    "\n",
    "target3_test_data=[]\n",
    "for i in range(target1_test_len+target2_test_len,target1_test_len+target2_test_len+target3_test_len):\n",
    "    target3_test_data.append(data_test_target3[\"Tweet\"][i])\n",
    "\n",
    "target4_test_data=[]\n",
    "for i in range(target1_test_len+target2_test_len+target3_test_len,target1_test_len+target2_test_len+target3_test_len+target4_test_len):\n",
    "    target4_test_data.append(data_test_target4[\"Tweet\"][i])\n",
    "    \n",
    "target5_test_data=[]\n",
    "for i in range(target1_test_len+target2_test_len+target3_test_len+target4_test_len,target1_test_len+target2_test_len+target3_test_len+target4_test_len+target5_test_len):\n",
    "    target5_test_data.append(data_test_target5[\"Tweet\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_test_data=convert_lowercase(target1_test_data)\n",
    "target1_test_data=remove_digits(target1_test_data)\n",
    "target1_test_data=remove_punctuations(target1_test_data)\n",
    "target1_test_data=tokenize_data(target1_test_data)\n",
    "target1_test_data=remove_stopwords(target1_test_data)\n",
    "target1_test_data=lemmatize(target1_test_data)\n",
    "target1_test_data=make_string(target1_test_data)\n",
    "\n",
    "target2_test_data=convert_lowercase(target2_test_data)\n",
    "target2_test_data=remove_digits(target2_test_data)\n",
    "target2_test_data=remove_punctuations(target2_test_data)\n",
    "target2_test_data=tokenize_data(target2_test_data)\n",
    "target2_test_data=remove_stopwords(target2_test_data)\n",
    "target2_test_data=lemmatize(target2_test_data)\n",
    "target2_test_data=make_string(target2_test_data)\n",
    "\n",
    "target3_test_data=convert_lowercase(target3_test_data)\n",
    "target3_test_data=remove_digits(target3_test_data)\n",
    "target3_test_data=remove_punctuations(target3_test_data)\n",
    "target3_test_data=tokenize_data(target3_test_data)\n",
    "target3_test_data=remove_stopwords(target3_test_data)\n",
    "target3_test_data=lemmatize(target3_test_data)\n",
    "target3_test_data=make_string(target3_test_data)\n",
    "\n",
    "target4_test_data=convert_lowercase(target4_test_data)\n",
    "target4_test_data=remove_digits(target4_test_data)\n",
    "target4_test_data=remove_punctuations(target4_test_data)\n",
    "target4_test_data=tokenize_data(target4_test_data)\n",
    "target4_test_data=remove_stopwords(target4_test_data)\n",
    "target4_test_data=lemmatize(target4_test_data)\n",
    "target4_test_data=make_string(target4_test_data)\n",
    "\n",
    "target5_test_data=convert_lowercase(target5_test_data)\n",
    "target5_test_data=remove_digits(target5_test_data)\n",
    "target5_test_data=remove_punctuations(target5_test_data)\n",
    "target5_test_data=tokenize_data(target5_test_data)\n",
    "target5_test_data=remove_stopwords(target5_test_data)\n",
    "target5_test_data=lemmatize(target5_test_data)\n",
    "target5_test_data=make_string(target5_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Stance value for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=[]\n",
    "for i in range(0,target1_train_len):\n",
    "    y1.append(data_train_target1[\"Stance\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating both Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target1_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d23efb86cdcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtarget1_full_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtarget1_full_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget1_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtarget1_full_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget1_test_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target1_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "target1_full_data=[]\n",
    "target1_full_data.extend(target1_train_data)\n",
    "target1_full_data.extend(target1_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TfIDF vectorizer for the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_train_1 = TfidfVectorizer()\n",
    "X = vectorizer_train_1.fit_transform(target1_full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733, 2869)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=vectorizer_train_1.transform(target1_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 2869)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperation of training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "clf2 = SVC(probability=True, kernel='rbf')\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate predictions\n",
    "predictions = clf2.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AGAINST', 'FAVOR', 'NONE'], dtype='<U7')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final=np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.zeros(170)\n",
    "for i in range(170):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 2 0 2 2 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 2 0 0 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 2 0 1 0 0 0 0 0 0 1 0 2 0 0 0 0 1 1 0 1 0 0 2 0 0\n",
      " 0 1 0 2 0 1 0 0 2 0 0 0 0 0 0 0 2 0 2 0 0 2 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5938973366390684"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, predictions_final, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### storing ground truth values for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_test_answers=pd.read_csv('dataset_test_answers.csv')\n",
    "data_test_target1_answers=data_test_answers[data_test_answers[\"Target\"]==\"Atheism\"]\n",
    "\n",
    "y_ground_truth=data_test_target1_answers[\"Stance\"].values;\n",
    "print(type(y_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target1_test=vectorizer_train_1.transform(target1_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 2869)\n"
     ]
    }
   ],
   "source": [
    "print(X_target1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate predictions\n",
    "predictions_test_1 = clf2.predict_proba(X_target1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_1=np.argmax(predictions_test_1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 2, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
       "       0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0,\n",
       "       2, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0,\n",
       "       0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 1, 2, 2, 2, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1, 0,\n",
       "       0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       2, 0, 2, 1, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 2, 1, 0, 1, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "y_ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of F1_Score , Precision_Score , Recall_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_ground_truth, predictions_test_1, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_1, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_1, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the same way, we calculate for other targets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2_full_data=[]\n",
    "target2_full_data.extend(target2_train_data)\n",
    "target2_full_data.extend(target2_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_train_2 = TfidfVectorizer()\n",
    "X_2 = vectorizer_train_2.fit_transform(target2_full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=vectorizer_train_2.transform(target2_train_data)\n",
    "X_target2_test=vectorizer_train_2.transform(target2_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 2660)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=[]\n",
    "for i in range(target1_train_len,target1_train_len+target2_train_len):\n",
    "    y1.append(data_train_target2[\"Stance\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "clf3 = SVC(probability=True, kernel='rbf')\n",
    "clf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AGAINST', 'FAVOR', 'NONE'], dtype='<U7')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate predictions\n",
    "predictions = clf3.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5740617119121793"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, predictions_final, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_test_target2_answers=data_test_answers[data_test_answers[\"Target\"]==\"Climate Change is a Real Concern\"]\n",
    "\n",
    "y_ground_truth=data_test_target2_answers[\"Stance\"].values;\n",
    "print(type(y_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and evaluate predictions\n",
    "predictions_test_2 = clf3.predict_proba(X_target2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_2=np.argmax(predictions_test_2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0,\n",
       "       2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 2, 1, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "y_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3729654023771671\n",
      "0.36356209150326796\n",
      "0.383894696089818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Udemy\\Anaconda\\envs\\irlab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_ground_truth, predictions_test_2, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_2, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949, 3541)\n",
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "1.0\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 1 1 1 0 1 2 2 0 2 1 1 2 0 2 2 0 0 1 2 0 2 0 1 0 2 2 1 2 0 0 0\n",
      " 0 2 1 1 2 1 1 2 0 0 2 0 1 2 0 1 0 0 0 0 2 0 1 2 0 2 0 0 0 2 1 0 1 1 0 1 0\n",
      " 2 0 2 0 1 2 0 2 0 0 2 2 1 2 2 2 1 1 0 1 2 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 1 1 1 1 2 1 2 2 1 2 0 2 0 0 1 0 0 0 1 1 2 1 2 0 2 0 0 0 1 0 1 0 0 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 2 0 0 0 0 0 1 0 0 2 0 2 0 1 2 1 2 1 1 1 1 1 0 1 0]\n",
      "[0 0 1 0 0 0 0 1 1 1 0 2 2 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 2 1 0 0 0 0\n",
      " 2 0 2 0 0 0 2 0 1 0 0 0 2 0 0 0 0 0 2 1 0 0 2 1 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 2 0 0 0 1 0 0 1 1 2 0 1 0 1 2 0 2 2 0 0 0 2 0 0 0 0 0 1 0 2 0 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 2 0 0 2 1 2 1 0 1 2 1 0 1 1 0 1 0\n",
      " 2 0 0 1 1 0 1 0 0 0 0 2 0 0 2 2 2 1 0 1 2 1 0 0 0 1 2 1 0 1 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 1 0 0 2 0 0 2 0 2 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1\n",
      " 0 1 1 0 1 1 0 0 0 2 2 0 0 0 0 1 0 0 2 2 2 1 0 0 0 0 0 0 2 0 2 0 0 0 0 2 0\n",
      " 1 1 0 0 1 0 1 1 0 0 0 0 2 0 1 0 0 2 1 0 1 2 1 0 0 0]\n",
      "0.517905973076751\n",
      "0.5197304651880784\n",
      "0.5294750786554066\n",
      "0.5789473684210527\n",
      "0.5789473684210527\n",
      "0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "target3_full_data=[]\n",
    "target3_full_data.extend(target3_train_data)\n",
    "target3_full_data.extend(target3_test_data)\n",
    "\n",
    "vectorizer_train_3 = TfidfVectorizer()\n",
    "X_3 = vectorizer_train_3.fit_transform(target3_full_data)\n",
    "\n",
    "print(X_3.shape)\n",
    "\n",
    "X1=vectorizer_train_3.transform(target3_train_data)\n",
    "X_target3_test=vectorizer_train_3.transform(target3_test_data)\n",
    "\n",
    "\n",
    "y1=[]\n",
    "for i in range(target1_train_len+target2_train_len,target1_train_len+target2_train_len+target3_train_len):\n",
    "    y1.append(data_train_target3[\"Stance\"][i])\n",
    "    \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "clf4 = SVC(probability=True, kernel='rbf')\n",
    "clf4.fit(X1, y1)\n",
    "\n",
    "\n",
    "print(clf4.classes_)\n",
    "\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions = clf4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;\n",
    "        \n",
    "        \n",
    "print(f1_score(y_true, predictions_final, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "data_test_target3_answers=data_test_answers[data_test_answers[\"Target\"]==\"Feminist Movement\"]\n",
    "\n",
    "y_ground_truth=data_test_target3_answers[\"Stance\"].values;\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;\n",
    "        \n",
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "print(type(y_ground_truth))\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions_test_3 = clf4.predict_proba(X_target3_test)\n",
    "\n",
    "predictions_test_3=np.argmax(predictions_test_3,axis=1)\n",
    "\n",
    "print(y_ground_truth)\n",
    "\n",
    "print(predictions_test_3)\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_3, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_3, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_3, average='macro'))\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_3, average='micro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_3, average='micro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_3, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 3317)\n",
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 2 2 0 2 0 2 0 2 2 0 0 0 2 2 0 2 2 0 2 2 2 0 1 2 2 2 1 2\n",
      " 2 2 2 2 1 0 2 1 2 2 1 1 1 2 1 0 2 1 2 0 2 1 1 0 1 1 0 1 1 2 2 2 0 1 2 1 0\n",
      " 2 2 0 1 0 1 0 0 0 1 0 0 0 0 2 0 1 2 1 0 1 0 0 2 1 2 2 2 0 2 0 0 0 1 0 1 0\n",
      " 2 0 0 2 0 2 2 0 1 2 0 0 2 0 1 0 1 0 2 1 1 0 0 0 2 2 0 0 0 2 0 0 0 2 2 2 0\n",
      " 2 0 0 0 0 0 1 1 0 0 0 2 1 2 0 2 0 0 0 0 2 0 2 0 0 0 0 0 2 2 1 2 0 0 2 0 0\n",
      " 1 0 0 1 1 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 2 2 2 2 0 0 2 0 2 1 0 0 2 0 2 0 0 0 2 2 0 1 2 1 0 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 1 1 0 1 0 0 0\n",
      " 0 0 2 0 0 0 0 0 2 0 0 0 2 0 0 0 2 2 0 0 0 2 0 2 0 0 0 0 2 2 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 2 0 0 2 1 0 0 2 1 2 0 0 1 0 1 0 0 2 0 0 1 0 0 0\n",
      " 2 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 1\n",
      " 2 2 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 2 0 0 2 2 2 2 2 0 0 0 2 1 0 0 0 0 0 1 0 1 0 2 0 0 0 0 0 2]\n",
      "0.5252349134074937\n",
      "0.6260792360013051\n",
      "0.5019810508182602\n",
      "0.6474576271186441\n",
      "0.6474576271186441\n",
      "0.6474576271186441\n"
     ]
    }
   ],
   "source": [
    "target4_full_data=[]\n",
    "target4_full_data.extend(target4_train_data)\n",
    "target4_full_data.extend(target4_test_data)\n",
    "\n",
    "vectorizer_train_4 = TfidfVectorizer()\n",
    "X_4 = vectorizer_train_4.fit_transform(target4_full_data)\n",
    "\n",
    "print(X_4.shape)\n",
    "\n",
    "X1=vectorizer_train_4.transform(target4_train_data)\n",
    "X_target4_test=vectorizer_train_4.transform(target4_test_data)\n",
    "\n",
    "\n",
    "y1=[]\n",
    "for i in range(target1_train_len+target2_train_len+target3_train_len,target1_train_len+target2_train_len+target3_train_len+target4_train_len):\n",
    "    y1.append(data_train_target4[\"Stance\"][i])\n",
    "    \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "clf4 = SVC(probability=True, kernel='rbf')\n",
    "clf4.fit(X1, y1)\n",
    "\n",
    "\n",
    "print(clf4.classes_)\n",
    "'''\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions = clf4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;\n",
    "        \n",
    "        \n",
    "print(f1_score(y_true, predictions_final, average='macro'))\n",
    "\n",
    "'''\n",
    "\n",
    "data_test_target4_answers=data_test_answers[data_test_answers[\"Target\"]==\"Hillary Clinton\"]\n",
    "\n",
    "y_ground_truth=data_test_target4_answers[\"Stance\"].values;\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;\n",
    "        \n",
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "print(type(y_ground_truth))\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions_test_4 = clf4.predict_proba(X_target4_test)\n",
    "\n",
    "predictions_test_4=np.argmax(predictions_test_4,axis=1)\n",
    "\n",
    "print(y_ground_truth)\n",
    "\n",
    "print(predictions_test_4)\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_4, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_4, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_4, average='macro'))\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_4, average='micro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_4, average='micro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_4, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883, 3097)\n",
      "(603, 3097)\n",
      "603\n",
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 1 2 0 2 2 2 2 1 0 0 2\n",
      " 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 1 2 2 1 2 2 0 1 0 0 1 1 2 0 2 0 2 1 2 1 0 2 1 2 0 1 2 1\n",
      " 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 0 2 0 0 0 2 0 1 0 0 2 0 0 2 0 1 0 0 1 1 2 1\n",
      " 1 0 2 1 0 0 1 1 0 0 0 2 1 1 0 0 0 1 0 0 1 2 0 0 2 0 0 2 2 0 2 0 0 2 1 0 0\n",
      " 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1\n",
      " 0 1 0 2 2 1 1 0 2 1 2 0 2 2 0 0 2 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 2 1 2 0 1 0 0 0 1 0 0 0 0 1 2 1 1 0 0 2 2 2 2 1 0 0 0\n",
      " 0 0 2 0 2 2 0 2 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 2 0 1 0 0 0 2 0 2 0 1 0 1 2\n",
      " 0 0 0 2 0 1 2 2 2 0 0 0 0 0 2 2 0 1 2 0 0 0 0 2 0 1 0 0 2 2 0 0 2 2 2 0 0\n",
      " 0 2 0 1 0 0 0 0 2 0 2 2 2 2 2 0 2 1 0 0 0 1 2 0 0 1 0 2 1 0 2 0 2 2 1 2 1\n",
      " 1 2 0 1 0 0 0 0 1 0 0 1 0 1 1 0 2 0 2 0 0 0 0 2 0 2 0 0 2 0 0 0 0 1 1 2 0\n",
      " 1 2 2 0 2 0 0 1 0 0 0 0 0 1 0 2 0 2 2 0 0 2 2 2 0 0 0 0 2 0 0 0 1 2 0 0 0\n",
      " 2 2 0 2 0 0 0 0 0 0 0 0 2 2 2 0 2 0 2 0 0 0 0 0 0 0 2 0 1 2 0 2 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 2 1 0 0 0 2 0 0 0 0 0 0 0]\n",
      "0.5526878959082347\n",
      "0.548540196485402\n",
      "0.5779234721263706\n",
      "0.6285714285714286\n",
      "0.6285714285714286\n",
      "0.6285714285714286\n"
     ]
    }
   ],
   "source": [
    "target5_full_data=[]\n",
    "target5_full_data.extend(target5_train_data)\n",
    "target5_full_data.extend(target5_test_data)\n",
    "\n",
    "vectorizer_train_5 = TfidfVectorizer()\n",
    "X_5 = vectorizer_train_5.fit_transform(target5_full_data)\n",
    "\n",
    "print(X_5.shape)\n",
    "\n",
    "X1=vectorizer_train_5.transform(target5_train_data)\n",
    "X_target5_test=vectorizer_train_5.transform(target5_test_data)\n",
    "\n",
    "\n",
    "y1=[]\n",
    "for i in range(target1_train_len+target2_train_len+target3_train_len+target4_train_len,target1_train_len+target2_train_len+target3_train_len+target4_train_len+target5_train_len):\n",
    "    y1.append(data_train_target5[\"Stance\"][i])\n",
    "\n",
    "print(X1.shape)\n",
    "print(len(y1))\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "clf5 = SVC(probability=True, kernel='rbf')\n",
    "clf5.fit(X1, y1)\n",
    "\n",
    "\n",
    "print(clf5.classes_)\n",
    "'''\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions = clf4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;\n",
    "        \n",
    "        \n",
    "print(f1_score(y_true, predictions_final, average='macro'))\n",
    "\n",
    "'''\n",
    "\n",
    "data_test_target5_answers=data_test_answers[data_test_answers[\"Target\"]==\"Legalization of Abortion\"]\n",
    "\n",
    "y_ground_truth=data_test_target5_answers[\"Stance\"].values;\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;\n",
    "        \n",
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "print(type(y_ground_truth))\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions_test_5 = clf5.predict_proba(X_target5_test)\n",
    "\n",
    "predictions_test_5=np.argmax(predictions_test_5,axis=1)\n",
    "\n",
    "print(y_ground_truth)\n",
    "\n",
    "print(predictions_test_5)\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_5, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_5, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_5, average='macro'))\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_5, average='micro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_5, average='micro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_5, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 2660)\n",
      "(395, 2660)\n",
      "395\n",
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "<class 'numpy.ndarray'>\n",
      "[2 1 1 2 1 1 1 1 2 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1\n",
      " 1 2 1 1 1 2 1 1 1 2 1 1 2 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 2 0 1 1 2 1 1 1 1 1 1 0 2\n",
      " 2 1 2 2 1 1 2 2 1 0 1 2 1 1 1 1 0 1 1 1 1 2 1 1 2 0 1 1 1 1 1 1 1 1 0 0 0\n",
      " 1 0 1 0 1 1 1 2 1 0 1 1 1 2 2 2 2 1 1 1 1]\n",
      "[1 2 1 1 1 1 2 2 2 1 1 2 2 1 2 1 1 1 1 1 1 2 1 2 1 1 1 1 1 2 1 1 2 1 1 1 1\n",
      " 1 2 1 2 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 2 2 1 1 2 1 2 1 1 1 1 2 1 2 1 2 2\n",
      " 2 2 1 2 1 1 1 2 1 2 2 2 2 2 1 1 1 1 1 1 1 1 2 1 2 2 1 1 2 1 1 1 2 1 1 1 2\n",
      " 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 2 1 2 1 1 1 1 1 1 2 1 2 1 2 1 1]\n",
      "0.3761789013125912\n",
      "0.3615686274509804\n",
      "0.39899341850561365\n",
      "0.6331360946745562\n",
      "0.6331360946745562\n",
      "0.6331360946745562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Udemy\\Anaconda\\envs\\irlab\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target2_full_data=[]\n",
    "target2_full_data.extend(target2_train_data)\n",
    "target2_full_data.extend(target2_test_data)\n",
    "\n",
    "vectorizer_train_2 = TfidfVectorizer()\n",
    "X_2 = vectorizer_train_2.fit_transform(target2_full_data)\n",
    "\n",
    "print(X_2.shape)\n",
    "\n",
    "X1=vectorizer_train_2.transform(target2_train_data)\n",
    "X_target2_test=vectorizer_train_2.transform(target2_test_data)\n",
    "\n",
    "\n",
    "y1=[]\n",
    "for i in range(target1_train_len,target1_train_len+target2_train_len):\n",
    "    y1.append(data_train_target2[\"Stance\"][i])\n",
    "\n",
    "print(X1.shape)\n",
    "print(len(y1))\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "clf2 = SVC(probability=True, kernel='rbf')\n",
    "clf2.fit(X1, y1)\n",
    "\n",
    "\n",
    "print(clf2.classes_)\n",
    "'''\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions = clf4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;\n",
    "        \n",
    "        \n",
    "print(f1_score(y_true, predictions_final, average='macro'))\n",
    "\n",
    "'''\n",
    "\n",
    "data_test_target2_answers=data_test_answers[data_test_answers[\"Target\"]==\"Climate Change is a Real Concern\"]\n",
    "\n",
    "y_ground_truth=data_test_target2_answers[\"Stance\"].values;\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;\n",
    "        \n",
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "print(type(y_ground_truth))\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions_test_2 = clf2.predict_proba(X_target2_test)\n",
    "\n",
    "predictions_test_2=np.argmax(predictions_test_2,axis=1)\n",
    "\n",
    "print(y_ground_truth)\n",
    "\n",
    "print(predictions_test_2)\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_2, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_2, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_2, average='macro'))\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_2, average='micro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_2, average='micro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733, 2869)\n",
      "(513, 2869)\n",
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 2 0 0 0 0 2 2 0 2 2 2 2 0 0 0 1 2 2 2 0 1 0 0 0 0\n",
      " 0 0 2 0 0 2 0 1 0 1 1 1 0 0 1 2 0 0 2 1 0 0 2 1 0 0 1 0 1 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 0 2 0 2 1 2 0 2 0 1 0 0 0 0 0 2 1 1 1 2 0 2 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 0 1 0 1 1 2 0 0 2 0 0 0 2 0 0 1 0 1 0]\n",
      "[0 0 0 1 0 0 1 0 2 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 1 1 2\n",
      " 2 0 0 0 0 0 0 0 2 2 0 1 2 1 1 1 1 2 1 0 0 0 0 0 0 2 0 2 0 0 2 0 0 0 1 0 2\n",
      " 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 1 2 0 0 2 2 2 2 0 1 0 2 0 2 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 2 0 0 0 0 0 2 0 1 2 0 1 0 2 1 2 0 2 0 0 0 0 1 1 0 1 0 2\n",
      " 2 0 0 0 0 0 2 0 1 1 2 0 2 0 1 0 0 0 0 0 0 1 0 0 2 1 1 2 0 0 0 0 0 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 0 0 0 0 0 2 0 0 2 0 0 0 0 0 2 0 0 0 0]\n",
      "0.560064935064935\n",
      "0.5459459459459459\n",
      "0.5857142857142857\n",
      "0.6954545454545454\n",
      "0.6954545454545454\n",
      "0.6954545454545454\n"
     ]
    }
   ],
   "source": [
    "target1_full_data=[]\n",
    "target1_full_data.extend(target1_train_data)\n",
    "target1_full_data.extend(target1_test_data)\n",
    "\n",
    "vectorizer_train_1 = TfidfVectorizer()\n",
    "X_1 = vectorizer_train_1.fit_transform(target1_full_data)\n",
    "\n",
    "print(X_1.shape)\n",
    "\n",
    "X1=vectorizer_train_1.transform(target1_train_data)\n",
    "X_target1_test=vectorizer_train_1.transform(target1_test_data)\n",
    "\n",
    "\n",
    "y1=[]\n",
    "for i in range(0,target1_train_len):\n",
    "    y1.append(data_train_target1[\"Stance\"][i])\n",
    "\n",
    "print(X1.shape)\n",
    "\n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# train classifier\n",
    "clf1 = SVC(probability=True, kernel='rbf')\n",
    "clf1.fit(X1, y1)\n",
    "\n",
    "\n",
    "print(clf1.classes_)\n",
    "'''\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions = clf4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "predictions_final=np.argmax(predictions,axis=1)\n",
    "\n",
    "\n",
    "y_true=np.zeros(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==\"AGAINST\"):\n",
    "        y_true[i]=0;\n",
    "    elif(y_test[i]==\"FAVOR\"):\n",
    "        y_true[i]=1;\n",
    "    else:\n",
    "        y_true[i]=2;\n",
    "        \n",
    "        \n",
    "print(f1_score(y_true, predictions_final, average='macro'))\n",
    "\n",
    "'''\n",
    "\n",
    "data_test_target1_answers=data_test_answers[data_test_answers[\"Target\"]==\"Atheism\"]\n",
    "\n",
    "y_ground_truth=data_test_target1_answers[\"Stance\"].values;\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_ground_truth)):\n",
    "    if(y_ground_truth[i]==\"AGAINST\"):\n",
    "        y_ground_truth[i]=0;\n",
    "    elif(y_ground_truth[i]==\"FAVOR\"):\n",
    "        y_ground_truth[i]=1;\n",
    "    else:\n",
    "        y_ground_truth[i]=2;\n",
    "        \n",
    "y_ground_truth=y_ground_truth.astype('int64')\n",
    "print(type(y_ground_truth))\n",
    "\n",
    "# predict and evaluate predictions\n",
    "predictions_test_1 = clf1.predict_proba(X_target1_test)\n",
    "\n",
    "predictions_test_1=np.argmax(predictions_test_1,axis=1)\n",
    "\n",
    "print(y_ground_truth)\n",
    "\n",
    "print(predictions_test_1)\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_1, average='macro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_1, average='macro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_1, average='macro'))\n",
    "\n",
    "print(f1_score(y_ground_truth, predictions_test_1, average='micro'))\n",
    "print(precision_score(y_ground_truth, predictions_test_1, average='micro'))\n",
    "print(recall_score(y_ground_truth, predictions_test_1, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
